[[chapter2]]

==  RISC-V security model overview

The aim of this chapter is to define common taxonomies and principles for
secure systems as used in the rest of this and other RISC V specifications. It
is divided into the following sections:

* Reference model +
Defines a set of generic hardware and software subsystems used in examples and
use cases to describe secure systems.

* Adversarial model +
Defines common attack types on secure systems, and identifies RISC-V extensions
which can aid mitigation.

* Ecosystem security objectives +
Defines common security features and functional guidelines, used to deploy
trustworthy devices in an ecosystem.

=== Reference model

[caption="Figure {counter:image}: ", reftext="Figure {image}"]
[title= "Generic security reference model"]
image::img_ch2_reference-model.png[]

The figure above outlines a generic security reference model. It is not
intended to describe any particular implementation and only aims to define a
common taxonomy for the purpose of this and other RISC-V specifications.

Most systems are made up of different software components, often from different
supply chains, each managing _assets_ that need to be protected.

==== Assets

Examples of assets include:

* Cryptographic keys and credentials
* User data
* Proprietary models
* Secret algorithms

In this specification, a _hardware provisioned asset_ is an immutable asset
provisioned in hardware by a security provisioning process, before a device is
used in a production environment. For example, hardware provisioned keys or
identities.

==== Trusted Computing Base (TCB)

The _Trusted Computing Base (TCB)_ of a component is the minimum amount of
software and hardware that needs to be trusted by that component.

The TCB of a component typically includes other software. For example:

* An operating system
* A hypervisor and a guest operating system
* A TEE security manager
* Hosting services such as orchestration and server provisioning software

==== Root of trust

The minimum amount of hardware and/or software that always has to be trusted on
a system is its _root of trust (RoT)_. A RoT supports fundamental security
services, for example:

* Boot and attestation
* Security lifecycle management
* Key derivations and sealing
* Security provisioning

Depending on use case and ecosystem requirements, a RoT can be:

* Hart firmware (FW RoT)
* A dedicated trusted subsystem (HW RoT), supporting a FW RoT

Using a HW RoT moves critical functions and assets off a Hart to a dedicated
trusted subsystem, which can provide stronger protection against physical and
logical attack than a complex Hart.

NOTE: It is common for secure systems to support multiple trust chains with
their own root of trust. For example, a TPM can be a root of trust for UEFI
boot flows within a runtime environment, and a SIM can be a root of trust for
user identity management. +
 +
For the purpose of this document, these should be treated as _secondary roots of
trust_. +
 +
The HW RoT acts as a _primary root of trust_ on the system. For example, the HW
RoT governs boot, and can load firmware and manage the security lifecycle of a
secondary root of trust.

[#cat_sr_sub_rot]
[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_ROT_001
| A complex secure system MUST implement a HW RoT

| SR_ROT_002
| A simpler system MAY not require a HW RoT but one is still recommended

|===

Examples of a complex system include one with coherent multi-core, or out of
order, application Harts.

Examples of a simpler system include a single core in-order microcontroller.

NOTE: In this document, the terms "FW RoT" and "HW RoT" will be used as defined
above. The term "RoT" on its own can be used where a rule or a rational applies
to either model.

==== Isolation

Assets can be protected by _isolation_. Isolation reduces dependencies between
components, and reduces the amount of software that needs to be trusted.

Isolation protects _resources_:

* Memory and memory mapped devices
* _Execution state_, including Hart register state

Examples of isolation mechanisms include:

* Privilege based isolation +
More privileged software is able to enforce security guarantees for less
privileged software.
* Physical memory isolation +
More privileged software controls memory access for less privileged software.
* Domain isolation +
Software in one domain cannot access or modify resources assigned to a different
domain (without consent), regardless of privilege level. +
(Higher privileged software in one domain cannot access resources assigned to a
lower privilege level in a different domain)
* Virtualization +
Virtualization creates and manages _virtual resources_ - compute, memory,
devices - independent of actual physical hardware. A system, or individual
domains, can be virtualized.

On complex systems the TCB can grow large and get difficult to certify and
attest.

Domain isolation enables confidential workloads to be separated from complex
hosting software, including other workloads. The TCB of a confidential workload
can be reduced to a domain security manager in a confidential domain, and the
RoT, while allowing the main runtime environment in a separate hosting domain
to remain in control of resource management.

Domain isolation use cases include:

* Platform security services - for example: secure storage, user identity
management, payment clients, DRM clients
* Hosted confidential third party workloads

==== Device assignment

Isolation policy needs to extend to device assignment:

* Physical memory access control for device initiated transactions
* Virtual memory translation for virtualized device transactions
* Interrupt management across privilege and domain boundaries

These policies can be enforced by system level hardware, controlled by Hart
firmware.

==== Invasive subsystems

_Invasive subsystems_ include any system or Hart feature which could
break security guarantees, either directly or indirectly. For example:

* External debug
* Power and timing management
* RAS (_reliability, accessibility, serviceability_)

[#cat_sr_sub_inv]
[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_INV_001
| Invasive subsystems MUST be controlled, or moderated, by a RoT.

| SR_INV_002
| Invasive subsystems SHOULD be enabled separately for M-mode &
non-M-mode software.
|===

==== Event counters

Event counters are commonly used for performance management and resource
allocation.

However, they can also pose a security risk. For example, one workload
monitoring an operation in a different workload, or an operation by higher
privilege software, could be able to reveal assets used in those operations.

[#cat_sr_sub_pmu]
[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_PMU_001
| Lower privileged software MUST NOT be able to monitor higher privileged
software.

| SR_PMU_002
| Software in one domain MUST NOT be able to monitor software in a different
domain, without consent.

|===

==== Platform quality of service

Server platforms can provide _platform quality of service (QoS)_ features,
consisting of Hart and system hardware and firmware aimed at managing access to
shared physical resources across workloads, minimizing contention. For example:

* Memory bandwidth management
* Cache allocation policies across workloads, including workload prioritization
* Hart allocation policies across workloads

These types of features rely on monitoring resource utilization of workloads,
similar to event counters, and optimizing resource allocation policies.

[#cat_sr_sub_qos]
[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_QOS_001
| Lower privileged software MUST NOT be able to monitor higher privileged
software.

| SR_QOS_002
| Software in one domains MUST NOT be able to monitor software in a different
domain, without consent.

|===

==== Denial of service

The RISC-V security model is primarily concerned with protection of assets.

For example, a hosting environment is free to apply their own resource
allocation policy to any workloads. Including denying service. This applies in
the same way to confidential workloads.

[#cat_sr_sub_dos]
[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_DOS_001
| Lower privileged software MUST NOT be able to deny service to higher
privileged software, or other isolated workloads at the same privilege level.

|===

Higher privileged software always has to be able to enforce its own resource
management policy without interference. Including scheduling, resource
assignment and recovation policies.

=== Adversarial model

For the purpose of this specification, the main goal of an adversary is to gain
unauthorized access to _resources_ - memory, memory mapped devices, and
execution state. For example, to access sensitive assets, to gain privileges,
or to affect the control flow of a victim.

In general, adversaries capable of mounting the following broad classes of
attacks should be considered by system designers:

* Logical +
The attacker and the victim are both processes on the same system.

* Physical +
The victim is a process on a system, and the attacker has physical access to
the same system. For example: probing, interposers, glitching, and disassembly.

* Remote +
The victim is a process on a system, and the attacker does not have physical or
logical access to the system. For example, radiation or power fluctuations, or
protocol level attacks on connected services.

Attacks can be direct or indirect:

* Direct +
An adversary gains direct access to a resource belonging to the victim. For
example: direct access to a memory location or execution state, or direct
control of the control flow of a victim.

* Indirect +
An adversary can access or modify the content of a resource by a side channel.
For example: by analyzing timing patterns of an operation by a victim to reveal
information about data used in that operation, or launching row-hammer style
memory attacks to affect the contents of memory owned by the victim.

* Chained +
An adversary is able to chain together multiple direct and indirect attacks to
achieve a goal. For example, use a software interface exploit to affect a call
stack, and use that to take redirect the control flow of a victim.

This specification is primarily concerned with ISA level mitigations against
logical attacks.

Physical or remote attacks in general need to be addressed at system, protocol
or governance level, and may require additional non-ISA mitigations. However,
some ISA level mitigations can also help provide some mitigation against
physical or remote attacks and this is indicated in the tables below.

The required level of protection can vary depending on use case. For example, a
HW RoT may have stronger requirements on physical resistance than other parts
of an SoC.

Finally, this specification does not attempt to rate attacks by severity, or by
adversary skill level. Ratings tend to depend on use case specific threat
models and requirements.

==== Logical

[#cat_sr_sub_lgc]
[width=100%]
[%header, cols="5,5,5,10,15,10"]
|===
| ID#
| Attack
| Type
| Description
| Current RISC-V mitigations
| Planned RISC-V mitigations

| SR_LGC_001
| Unrestricted access
| Direct +
Logical
| Direct access to unauthroized resources in normal operation.
a| * RISC-V privilege levels
* RISC-V isolation (for example: PMP/sPMP, MTT, supervisor domains)
* RISC-V hardware virtualization (H extension, MMU)
|

| SR_LGC_002
| Transient execution attacks
| Chained +
Logical
| Attacks on speculative execution implementations.
| Known (documented) attacks, except Spectre v1, are specific to particular
micro-architectures. Micro-architecture for RISC-V systems is implementation
specific, but must not introduce such vulnerabilities. +
 +
This is an evolving area of research. +
 +
 For example: +
https://meltdownattack.com/[Spectre and meltdown papers] +
https://www.intel.com/content/www/us/en/developer/topic-technology/software-security-guidance/processors-affected-consolidated-product-cpu-model.html[Intel
security guidance] +
https://developer.arm.com/documentation/#cf-navigationhierarchiesproducts=Arm%20Security%20Center,Speculative%20Processor%20Vulnerability[Arm speculative
vulnerability]
| Fence.t, or similar future extensions, could at least partially mitigate against Spectre v1.

| SR_LGC_003
| Interface abuse
| Chained +
Logical
| Abusing interfaces across privilege or isolation boundaries, for example to
elevate privilege or to gain unauthorized access to resources.
a| * RISC V privilege levels
* RISC-V isolation
| High assurance cryptography

| SR_LGC_004
| Event counting
| Direct +
Logical
| For example, timing processes across privilege or isolation boundaries to
derive information about confidential assets.
a| * Data-independent timing instructions
* Performance counters restricted by privilege and isolation boundaries
(sscofpmf, smcntrpmf)
|

| SR_LGC_005
| Redirect control flow
| Chained +
Logical
| Unauthorized manipulation of call stacks and jump targets to redirect a
control flow to code controlled by an attacker.
a| * Shadow stacks (Zicfiss)
* Landing pads (Zicfilp)
|

|===

==== Physical and remote

[#cat_sr_sub_phy]
[width=100%]
[%header, cols="5,10,10,15,15"]
|===
| ID#
| Attack
| Type
| Description
| RISC-V recommendations

| SR_PHY_001
| Analysis of physical leakage
| Direct or indirect +
Physical or remote
| For example, observing radiation, power line patterns, or temperature.
a| * Implement robust power management and radiation control
* Data Independent Execution Latency (Zkt, Zvkt)

| SR_PHY_002
| Physical memory manipulation
| Direct +
Logical or physical
a| * Row-hammer type software attacks to manipulate nearby memory cells
* Using NVDIMM, interposers, or physical probing to read, record, or replay
physical memory
* Physical attacks on hardware shielded locations to extract hardware
provisioned assets
a| * Implement robust memory error detection, cryptographic memory protection,
or physical tamper resistance
* Supervisor domain ID, privilege level, or MTT attributes, could be used to
derive memory encryption contexts at domain or workload granularity
* Provide a degree of tamper resistance

| SR_PHY_003
| Boot attacks
| Chained +
Logical or physical
a| * Glitching to bypass secure boot
* Retrieving residual confidential memory after a system reset
a| * Implement robust power management
* Implement cryptographic memory protection with at least boot freshness

| SR_PHY_004
| Subverting supply chains
| Remote
| Infiltration or collusion to subvert security provisioning chains, software
supply chains and signing processes, hardware supply chains, attestation
processes, development processes (for example, unfused development hardware or
debug authorizations)
| Deploy appropriate governance, accreditation, and certification processes for
an ecosystem.

|===

=== Ecosystem security objectives

Ecosystem security objectives identify a set of common features and mechanisms
that can be used to enforce and establish trust in an ecosystem.

These features are defined here at a functional level only. Technical
requirements are typically use case specific and defined by external
certification programmes.

In some cases RISC-V non-ISA specifications can provide guidance or protocols.
This is discussed more in use case examples later in this specification.

==== Secure identity

[cat_sr_sub_idn]
[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_IDN_001
| A security platform MUST be securely identifiable
|===

Identifies the immutable part of the security platform - immutable hardware,
configurations, and firmware. Immutable components cannot change after
completed security provisioning (see also security lifecycle management).

A _secure identity_ is one capable of generating a cryptographic signature
which can be verified by a remote party. Usually an asymmetric key pair, but
sometimes symmetric signing schemes can be used). It is typically used as part
of an attestation process.

Its scope and uniqueness depends on use case. For example:

* Unique to a system
* Shared among multiple systems with the same immutable security properties
(group based anonymization)
* Anonymized using an attestation protocol supporting a third party
anonymization service

It can be directly hardware provisioned, or derived from other hardware
provisioned assets.

==== Security lifecycle

[#cat_sr_sub_lfc]
[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_LFC_001
| A secure system MUST manage a security lifecycle.
|===

[caption="Figure {counter:image}: ", reftext="Figure {image}"]
[title= "Generic security lifecycle"]
image::img_ch2_security-lifecycle.png[]

[#security-lifecycle]
A security lifecycle reflects the trustworthiness of a system during its
lifetime and reflects the lifecycle state of hardware provisioned assets.

It can be extended as indicated below to cover additional security provisioning
steps such as device onboarding, device activation, user management, and RMA
processes. These are use case or ecosystem specific and out of scope of this
specification.

For the purpose of this specification, _revealing debug_ includes any HW or FW
debug capability which

* Could break security guarantees or could expose assets
* Is not part of an attested trust contract with a relying party

Examples of revealing debug include revealing logging, external debug or
boundary scans, dedicated debug builds of software components, or enabling
self-hosted debug for a component.

Depending on use case, an attested software component can include debug
capabilities managed through an ecosystem defined governance process
- _trusted debug_. For example, self-hosted debug enabled following an ecosystem
specific authorization process. In this case the debug capability, and the
associated governance, is part of the trust contract with a relying party.

For the purpose of this specification, a minimum security lifecycle includes at
least the following states:

* Manufacture - The system may not yet be locked down and has no hardware
provisioned assets
* Security provisioning - The process of provisioning hardware provisioned
assets +
Depending on ecosystem requirement, security provisioning could be performed in
multiple stages through a supply chain and may require additional sub-states.
These types of application specific extensions are out of scope of this
specification.
* Secured - hardware provisioned assets are locked (immutable), only authorized
software can be used, and revealing debug is not enabled. +
Additional specific provisioning stages can take place in this
state - for example network onboarding and device activation, TSS/App/Device
attestation or user identity management. This is out of scope of this
specification.
* Recoverable debug - part of the system is in a revealing debug state +
At least the RoT is not compromised and hardware provisioned secrets remain
protected. +
This state is both attestable and recoverable. For example, revealing debug is
enabled for a domain without compromising another domain or any RoT services.
* Terminated - any system change which could expose hardware provisioned assets
+
Typically hardware provisioned assets are made permanently inaccessible and
revoked before entering this state. This also protects any derived assets such
as attestation and sealing keys.

A system could support re-provisioning from a terminated state, for example
following repair/RMA. This can be viewed as equivalent to starting over from the
security provisioning state, and creates a new instance with a new secure
identifier.

[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_LFC_002
| Hardware provisioned assets MUST only be accessible while the system is in
secured state, or a recoverable debug state.

| SR_LFC_003
| Derived assets MUST only be available if a component is in secured state.
|===

[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_LFC_004
| Hardware provisioned assets MUST only be accessible while the system is in
secured state, or a recoverable debug state (with the recoverable debug state in
attestation evidence).

| SR_LFC_005
| Derived assets MUST only be available if a component is in secured state.
|===

A derived asset in this context is any asset derived from hardware provisioned
assets. For example attestation keys, or sealing keys for a supervisor domain.

[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_LFC_006
| Revealing debug MUST be reflected in attestation.

|===

_Attestable states_ are ones where the RoT and hardware provisioned assets are
not compromised by debug and a valid attestation can be generated reflecting
that state:

* Secured
* Recoverable debug

In other states the system is not able to generate a valid attestation key. It
is still _indirectly attestable_ as any generated attestation will not be signed
correctly and can be rejected by a relying party.

Trusted debug is part of a trust contract with a relying party, and application
specific. The presence of trusted debug can be determined indirectly by a
relying party through other attested properties, for example measurements.

==== Attestable services

For the purpose of this specification a confidential service can be any
isolated component on a system. For example, a hosted confidential workload, or
an isolated application security service.

[#cat_sr_sub_att]
[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_ATT_001
| A confidential service, and all software and hardware components it depends
on, MUST be attestable.
|===

Attestation allows a remote relying party to determine the trustworthiness of a
confidential service before submitting assets to it.

* Verify the security state of a confidential service
* Verify the security state of all software and hardware a conidential service
depends on
* Establish an attested secure connection to a confidential service

Attestation can be direct or layered.

* Direct +
The whole system can be defined by a single security platform attestation. For
example, can be used in vertically integrated connected IoT devices and edge
devices.
* Layered +
Enables parts of the attestation process to be delegated to lower privileged
components.

Direct and layered attestation are discussed in more detail in use case
examples later in this specification.

[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_ATT_002
| A security platform attestation MUST be signed by a HW RoT, if present, or
else by a FW RoT

| SR_ATT_003
| A security platform attestation MUST be signed using a hardware provisioned
(directly or derived) secure identity

| SR_ATT_004
| A layered attestation MAY be signed by lower privileged software, itself
attested by a security platform attestation

| SR_ATT_005
a| Layered attestations MUST be cryptographically bound such that a relying
party can determine that they:

* Were generated on the same system
* Are fresh.

|===

NOTE: Software interfaces should only support either direct attestation or
layered attestation workflows, never both, to prevent impersonation attacks.

==== Authorized software

Running unauthorized software can compromise the security state of the system.

[#cat_sr_sub_aut]
[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_AUT_001
| A system in secured or recoverable debug states MUST only load authorized
software.

| SR_AUT_002
| A system in security provisioning state SHOULD only load authorized software.

|===

Two complementary processes can be used to authorize software:

* Measuring +
A measurement is a cryptographic fingerprint, such as a running hash of memory
contents and launch state.
* Verification +
Verification is a process of establishing that a measurement is correct
(expected)

A boot process is typically layered, allowing software to be measured and
verified in stages. Different measurement and verification policies can be
employed at different stages. This is discussed further in use case examples
later in this specification. The properties discussed below still apply to each
stage.

NOTE: Measurements can be calculated at boot (_boot state_), and sometimes also
dynamically at runtime (_runtime state_). Measuring runtime state can be used as
a robustness feature to mitigate against unauthorized runtime changes of static
code segments. It is out of scope of this specification, though the principles
discussed below can still be applied.

Verification can be:

* Local +
A measurement is verified locally on the device.
* Remote +
A measurement is verified by a remote provisioning service, or a remote relying
party.

Verification can be:

* Direct +
The measurement is directly compared with an expected measurement from a signed
authorization.
* Indirect +
The measurement is included in derivations of other assets, for example sealing
keys, binding assets to a measured state.

[#cat_sr_sub_msm]
[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_MSM_001
| A security platform MUST be measured.

| SR_MSM_002
| A security platform MUST be verified, either directly or indirectly, before
launching services which depend on the security platform.

|===

Verification ensures the system has loaded authorized software

[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_MSM_003
| A system MUST only use authorizations from trusted authority.
|===

* Direct verification requires a signed image authorization from a trusted
authority before loading an image +
For example, a signed image, or a separately signed authorization
message.
* Indirect verification requires a signed authorization from a trusted authority
for migrating assets bound to a previously measured state +
For example, a signed provisioning message.

Either way, only authorizations from trusted authorities should be used. For
example, from a list of hardware provisioned or securely discovered trusted
authorities.

[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_MSM_004
| Local verification MUST be rooted in immutable boot code.
|===

For example, ROM or locked flash, or rooted in a HW RoT itself rooted in
immutable boot code.

==== System updates

Over time, any non-immutable component may need updates to address
vulnerabilities or functionality improvements. A system update can concern
software, firmware, microcode, or any other updatable component on a system.

[#cat_sr_sub_upd]
[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_UPD_001
| All components on a system which are not immutable MUST be updatable.
|===

Immutable components include at least immutable boot code. Some trusted
subsystems can also include immutable software to meet specific security
certification requirements.

System updates are typically layered so that updates can target only parts of a
system and not a whole system. The properties discussed below still apply to
any system update.

[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_UPD_002
| A system update MUST be measured and verified before launch.
|===

See <<_authorized_software>>.

A system update can be:

* Deferred +
The update can only be effected after a restart of at least the affected
component, and all of its dependents.
* Live +
The update can be effected without restarting any dependent components.

[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_UPD_003
| Updates affecting a security platform SHOULD be deferred.

| SR_UPD_004
| Updates MAY be live if live update capability, and suitable governance, is
part of an already attested trust contract between a relying party and the
system.
|===

A system update changes the attested security state of the affected
component(s), as well as that of all other components that depend on it. It can
affect whether a dependent confidential service is still considered trustworthy
or not, as well as affect any derived assets such as sealing keys.

[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_UPD_005
| System updates MUST be monotonic

| SR_UPD_006
| System updates SHOULD be robust against update failures
|===

Earlier versions may be carrying known vulnerabilities, or may affect the safe
operation of a system in other ways.

For example, using derived anti-rollback counters (counter tree) rooted in a
hardware monotonic counter.

A system can still support recovery mechanisms, with suitable governance, in
the case of update failures. For example, a fallback process or a dedicated
recovery loader.

Success criteria for a system update are typically use case or ecosystem
specific and out of scope of this specification. Examples include local
watchdog or checkpoints, and network control through a secure update protocol,
and a dedicated recovery loader.

[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_UPD_007
| System updates, and authorization messages, SHOULD only be received from
trusted sources.

|===

A system update is itself always verified before being launched. Verifying the
source as well can mitigate against attempts to inject adversary controlled
data into a local update process. Including into protected memory regions.

==== Isolation
Complex systems include software components from different supply chains, and
complex integration chains with different roles and actors. These supply chains
and integration actors often share mutual distrust:

* Developed, certified, deployed and attested independently
* Protected from errors in, or abuse from, other components
* Protected from debugging of other components
* Contain assets which should not be available to other components

Use cases later in this specification provide examples of RISC-V isolation
models.

[#cat_sr_sub_iso]
[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_ISO_001
| Isolated software components SHOULD be supported
|===

An isolated component has private memory and private execution contexts not
accessible to other components.

[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_ISO_002
| Devices MUST not access memory belonging to an isolated component without
permission
|===

Isolation can also extend to other features, such as interrupts and debug.

==== Sealing

Sealing is the process of protecting confidential assets on a system, typically
using sealing keys derived in different ways for different use cases as
discussed in this section. For example, from a hardware provisioned root key,
from a boot state (measurements, security lifecycle state), or provisioned at
runtime by a remote provisioning system.

Sealing can be:

* Local +
Local sealing binds assets to a local device (hardware unique sealing) or to a
measured boot state.
* Remote +
Remote sealing binds assets to credentials provided by a remote provisioning
service following successful attestation.

[#cat_sr_sub_slg]
[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_SLG_001
| Sealed assets SHOULD only be possible to unseal in a secured state

|===

For example, local sealing key derivations should take the security lifecycle
state of the system into account. And remote sealing key provisioning should
always attest the system before releasing unsealing credentials or keys.

Local sealing can be:

* Direct +
Direct sealing binds assets to sealing keys derived by a RoT.
* Layered +
 Layered sealing enables delegation of some sealing key derivations to lower
privileged software.

[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_SLG_002
| Locally sealed assets MUST only be possible to unseal on the same physical
instance of a system that they were sealed on.

|===

For example, using sealing keys derived from a hardware provisioned _hardware
unique key (HUK)_.

[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_SLG_003
| Locally sealed assets bound to a boot measurement MUST only be possible to
unseal if that measurement has not changed, or the system has received an
authorized update.

|===

See <<_system_updates, system updates>>

Sealing is discussed further in use cases examples later in this document.
