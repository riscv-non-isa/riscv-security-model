[[chapter2]]

==  RISC-V security model overview

The aim of this chapter is to define common taxonomies and principles for
secure RISC-V systems as used in the rest of this specification as well as other RISC-V specifications. It
is divided into the following sections:

* Reference model +
Defines a set of generic hardware and software subsystems used in examples and
use cases to describe secure systems.

* Adversarial model +
Defines common attack types on secure systems, and identifies RISC-V extensions
which can aid mitigation.

* Ecosystem security objectives +
Defines common security features and functional guidelines, used to deploy
trustworthy devices in an ecosystem.

=== Reference model

[caption="Figure {counter:image}: ", reftext="Figure {image}"]
[title= "Generic security reference model"]
image::img_ch2_reference-model.png[]

The figure above outlines a generic security reference model and taxonomy. This specification 
is oriented around this reference model. The model is not tied to any particular implementation.

Most systems are built with multiple software components each managing _assets_
that need to be protected. These components are often sourced from multiple
different supply chains. The security reference model uses the generic term
_domain_ to identify a logically isolated region, with at least some private 
resources and execution state not accessible to at least some other domain(s).
The figure above uses two domains for illustrative purposes but some
use-cases can require more. This specification does not imply or limit the number
of domains or the type of use cases a RISC-V system can support. See
xref:chapter4.adoc[Use case examples] for more comprehensive and specific examples.

Logically isolated domains at the software level are typically reflected in logically isolated domains at the system hardware level, where hardware resources can be assigned or restricted to specific software domains. For example, device DMA transfers can be restricted to memory assigned to a particular domain.

==== Assets

Examples of assets include:

* Cryptographic keys and credentials
* User data
* Proprietary models
* Secret algorithms

In this specification, a _hardware provisioned asset_ is an immutable asset
provisioned in hardware by a security provisioning process, before a device is
used in a production environment. For example, hardware provisioned keys or
identities.

==== Trusted Computing Base (TCB)

The _Trusted Computing Base (TCB)_ of any system function is the totality of
protection mechanisms within a computer system, including hardware,
firmware, and software - the combination responsible for enforcing a security
policy.

The following are examples of software components that can be a part of some function's TCB:

* An operating system
* A hypervisor and a guest operating system
* A TEE security manager
* Hosting services such as orchestration and server provisioning software

==== Root of trust

A _root of trust (RoT)_ is the foundation on which all secure operations of a system depend. A RoT is typically a combination of a minimal amount of hardware and software that has to be implicitly trusted by all system components.

A RoT supports fundamental security services, for example:

* Boot and attestation
* Security life cycle management
* Key derivations and sealing (sealing is defined in a later section)
* Security provisioning

Depending on use case and ecosystem requirements, a RISC-V RoT can be:

* Hart firmware (FW RoT)
* A dedicated trusted subsystem (HW RoT) supporting a FW RoT

Using a HW RoT moves critical functions and assets off a Hart to a dedicated and possibly isolated trusted subsystem, which can
provide stronger protection against physical and logical attacks.

The HW RoT acts as a _primary root of trust_ on the system.

NOTE: It is common for secure systems to support multiple trust chains with
their own root of trust. For example, a TPM can be a root of trust for UEFI
boot flows within a runtime environment while a SIM can be a root of trust for
user identity management. +
 +
For the purpose of this document, these should be treated as _secondary roots of
trust_. +
 +
The HW RoT can manage the security life cycle of a secondary root of trust (booting etc).

[#cat_sr_sub_rot]
[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_ROT_001
| A complex secure system MUST implement a HW RoT

| SR_ROT_002
| A simpler system MAY not require a HW RoT but one is still recommended

|===

An example of a complex system is one with multiple, out-of-order, cache-coherent harts.

An example of a simpler system is one with a single, in-order, hart such as in a microcontroller.

NOTE: In this document, the terms "FW RoT" and "HW RoT" will be used as defined
above. The term "RoT" on its own can be used where a rule or a rationale applies
to either model.

==== Isolation

Assets can be protected by _isolation_. Isolation reduces dependencies between
components, and reduces the amount of software that needs to be trusted.

Isolation protects _resources_:

* Memory and memory mapped devices
* _Execution state_, including Hart register state

Examples of isolation mechanisms include:

* Privilege based isolation +
More privileged software is able to enforce security guarantees for less
privileged software.
* Physical memory isolation +
More privileged software controls memory access for less privileged software.
* Domain isolation +
Software in one domain cannot access or modify resources assigned to a different
domain (without consent), regardless of privilege level. +
(Higher privileged software in one domain cannot access resources assigned to a
lower privileged software in a different domain)
* Virtualization +
Virtualization creates and manages _virtual resources_ - compute, memory,
devices - independent of actual physical hardware. A system, or individual
domains, can be virtualized.

On complex systems the TCB can grow large and become difficult to certify and
attest.

Domain isolation enables confidential workloads to be separated from complex
hosting software, including other workloads. The TCB of a confidential workload
can be reduced to a domain security manager in a confidential domain, and the
RoT, while allowing the main runtime environment in a separate hosting domain
to remain in control of resource management.

Examples of confidential workloads include:

* Platform security services - for example: secure storage, user identity
management, payment clients, DRM clients
* Hosted confidential third party workloads

RISC-V has a range of isolation mechanisms available and in development.

[#cat_sr_sub_iso]
[width=100%]
[%header, cols="10,25,5,5,5,10"]
|===
| Technololgy
| Use Case
| Privilege level
| Memory 
| Granularity
| Limitations

| PMP, ePMP
| Boot code isolation,  code and data isolation by privilege level. +
 Building block for simple trusted execution isolation using high privilege security monitor
| M
| Physical
| Fine Grained
| Switching overhead, limited resource

| SPMP
| OS managed code and data isolation by privilege level. +
 Building block to allow multiple OS to manage U mode isolation
| S
| Physical
| Fine Grained
| Switching overhead, limited resource

| Virtual Memory
MMU
| S - U,  U - U isolation +
Guest – Guest isolation (VS–VS) +
Host – Guest isolation (HS-VS)
| S +
HS/VS
| Virtual
| Page Based
|

| IOPMP
| System Level PMP
| n/a
| Physical
| Page Based
|

| Pointer Masking
| Simple SW based memory tagging, memory range restriction
| S U
| Both
| Coarse
|

| Smmpt, SDID
| Supervisor domains and memory proteciton tables, building block for confidential computing, trusted execution. +
S-S isolation
| S
| Physical
| Page or larger
| 

| Hardware Fault Isolation
| Simple memory range based task isolation. Accelerates isolation of containers for webasm etc. 
| U
| Virtual
| Fine Grained
| 

| Memory Tagging
| Faults on access to an incorrect TAG. 
used for debug, garbage collection, security isolation 
| S U
| Virtual
| tbd
| Probabilistic, performance impact, +
tag storage overhead

| CHERI
| Full Capability based access for memory safety and isolation
| M S U
| Both
| Fine Grained
| HW/SW impact

|===

==== Device assignment

Isolation policy needs to extend to device assignment:

* Physical memory access control for device initiated transactions
* Virtual memory translation for virtualized device transactions
* Interrupt management across privilege and domain boundaries

These policies can be enforced by system level hardware, controlled by Hart
firmware.

==== Invasive subsystems

_Invasive subsystems_ include any system or Hart feature which could
break security guarantees, either directly or indirectly. For example:

* External debug
* Power and timing management
* RAS (_reliability, accessibility, serviceability_)

[#cat_sr_sub_inv]
[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_INV_001
| Invasive subsystems MUST be controlled, or moderated, by a RoT.

| SR_INV_002
| Invasive subsystems SHOULD be enabled separately for M-mode &
non-M-mode software.

| SR_INV_003
| Invasive subsystems SHOULD be enabled separately for individual domains

|===

==== Event counters

Event counters are commonly used for performance management and resource
allocation on systems.

However, they can pose a security risk. For example, a workload can maliciously attempt to infer another workload's
secrets by monitoring that other workload's operation. The victim workload can be at the same, lower, or higher
privilege than the malicious workload.

[#cat_sr_sub_pmu]
[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_PMU_001
| Lower privileged software MUST NOT be able to monitor higher privileged
software.

| SR_PMU_002
| Software in one domain MUST NOT be able to monitor software in a different
domain, without consent.

|===

==== Platform quality of service

More complex systems, such as server platforms, can provide _platform quality of service (QoS)_ features beyond the capabilities of basic event counters. Platform QoS features include any Hart and system hardware and firmware aimed at managing access to
shared physical resources across workloads while minimizing contention.

For example:

* Memory bandwidth management
* Cache allocation policies across workloads, including workload prioritization
* Hart allocation policies across workloads

These types of features rely on monitoring the resource utilization of workloads,
similar to event counters, and on the optimization of resource allocation policies.

[#cat_sr_sub_qos]
[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_QOS_001
| Lower privileged software MUST NOT be able to observe QoS events or attributes concerning higher privileged
software.

| SR_QOS_002
| Software in one domain MUST NOT be able to observe QoS events or attributes concerning a different
domain, without consent.

|===

==== Denial of service

The RISC-V security model is primarily concerned with protection of assets. It is not concerned with providing service
guarantees.

For example, a hosting environment is free to apply its own resource allocation policy to relevant workloads. This can
include denying service to some workloads. 

[#cat_sr_sub_dos]
[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_DOS_001
| Lower privileged software MUST NOT be able to deny service to higher
privileged software, or other isolated workloads at the same privilege level.

| SR_DOS_002
| Software in one domain SHOULD NOT be able to deny service to software in a different domain

|===

Higher privileged software must always be able to enforce its own resource
management policy without interference, including scheduling, resource
assignment and revocation policies.

Similarly, a hosting domain owning resource allocation and host management across a system normally has to be able to enforce its own policies across domains. Including denying service. But other domains should not be able to deny service to the hosting domain, or to other domains.

=== Adversarial model

For the purpose of this specification, the main goal of an adversary is to gain
unauthorized access to _resources_ - memory, memory mapped devices, and
execution state. For example, to access sensitive assets, to gain privileges,
or to affect the control flow of a victim.

In general, adversaries capable of mounting the following broad classes of
attacks should be considered by system designers:

* Logical +
The attacker and the victim are both processes on the same system.

* Physical +
The victim is a process on a system, and the attacker has physical access to
the same system. For example: probing, interposers, glitching, and disassembly.

* Remote +
The victim is a process on a system, and the attacker does not have physical or
logical access to the system. For example, radiation or power fluctuations, or
protocol level attacks on connected services.

At an implementation level there can be further distinctions, for example the degree of proximity required to execute a remote or a physical attack as defined above. However, this document does not make any finer grained distinctions other than logical, physical and remote.

Attacks can be direct, indirect or chained:

* Direct +
An adversary gains direct access to a resource belonging to the victim. For
example: direct access to the victim's memory or execution state, or direct
control of the victim's control flow.

* Indirect +
An adversary can use a side channel to access or modify the content of a resource owned by the victim.
For example: by analyzing timing patterns of an operation by a victim to reveal
information about data used in that operation, or launching row-hammer style
memory attacks to affect the contents of memory owned by the victim.

* Chained +
An adversary is able to chain together multiple direct and indirect attacks to
achieve a goal. For example, using a software interface exploit to affect the call
stack such that control flow is redirected to the adversary's code.

The threats considered in-scope and the required level of protection will vary depending on use case. For example, a HW RoT would likely have a large set of threats that are considered applicable. Mitigating these threats may require protection against complex or advanced physical attacks. A Software based TEE may limit the threats considered applicable, and therefore the required mitigations. 

This specification is primarily concerned with ISA level mitigations against logical attacks.

Physical or remote attacks in general need to be addressed at system, protocol or governance level, and can require additional non-ISA mitigations. However, some ISA level mitigations can also help provide some mitigation against physical or remote attacks and this is indicated in the tables below.

Finally, this specification does not attempt to rate attacks by severity, or by adversary skill level. Ratings tend to depend on use case specific threat models and requirements.


==== Logical

[#cat_sr_sub_lgc]
[width=100%]
[%header, cols="5,5,5,10,15,10"]
|===
| ID#
| Threat
| Type
| Description
| Current RISC-V mitigations
| Planned RISC-V mitigations

| T_LGC_001
| Unrestricted access
| Direct +
Logical
| Unauthorized direct access to resources in normal operation.
a| * RISC-V privilege levels
* RISC-V isolation (for example: PMP/ePMP, sPMP, MTT, supervisor domains)
* RISC-V hardware enforced virtualization (H extension, MMU)
| CHERI

| T_LGC_002
| Transient execution attacks
| Chained +
Logical
| Attacks on speculative execution implementations.
| Known (documented) attacks, except Spectre v1, are specific to particular
micro-architectures. Micro-architecture for RISC-V systems is implementation
specific, but must not introduce such vulnerabilities. +
 +
This is an evolving area of research. +
 +
 For example: +
https://meltdownattack.com/[Spectre and meltdown papers] +
https://www.intel.com/content/www/us/en/developer/topic-technology/software-security-guidance/processors-affected-consolidated-product-cpu-model.html[Intel
security guidance] +
https://developer.arm.com/documentation/#cf-navigationhierarchiesproducts=Arm%20Security%20Center,Speculative%20Processor%20Vulnerability[Arm speculative
vulnerability]
| Fence.t, or similar future extensions, may at least partially mitigate against Spectre v1.

| T_LGC_003
| Interface abuse
| Chained +
Logical
| Abusing interfaces across privilege or isolation boundaries, for example to
elevate privilege or to gain unauthorized access to resources.
a| * RISC-V privilege levels
* RISC-V isolation
| High assurance cryptography

| T_LGC_004
| Event counting
| Direct +
Logical
| For example, timing processes across privilege or isolation boundaries to
derive information about confidential assets.
a| * Data-independent timing instructions
* Performance counters restricted by privilege and isolation boundaries
(sscofpmf, smcntrpmf)
|

| T_LGC_005
| Redirect control flow
| Chained +
Logical
| Unauthorized manipulation of call stacks and jump targets to redirect a
control flow to code controlled by an attacker.
a| * Shadow stacks (Zicfiss)
* Landing pads (Zicfilp)
| CHERI, +
  Memory Tagging

| T_LGC_006
| Memory safety
| Logical
| Unauthorized access to resources within an isolated component. For example, pointer or allocation errors (temporal memory safety), or buffer overflows (spatial memory safety).
a| * RISC-V pointer masking (J-extension) +
* Shadow stacks (Zicfiss) +
* Landing pads (Zicfilp) +
 +
Memory safe programming, for example: +
https:/www.cisa.gov/sites/default/files/2023-12/CSAC_TAC_Recommendations-Memory-Safety_Final_20231205_508.pdf +
| Architectural sandboxing, such as HFI. +
Capability based architecture, such as CHERI.

| T_LGC_007
| Architectural Covert Channel
| Logical
| Execution environment is unaware of, or doesnt swap/sanitize CSRs on context switch, creating covert communication channel between user threads or guest OSs
a| * Smstateen +
* Ssstateen
|

|===

==== Physical and remote

[#cat_sr_sub_phy]
[width=100%]
[%header, cols="5,10,10,15,15"]
|===
| ID#
| Threat
| Type
| Description
| RISC-V recommendations

| T_PHY_001
| Analysis of physical leakage
| Direct or indirect +
Physical or remote
| For example, observing radiation, power line patterns, or temperature.
a| * Implement robust power management and radiation control
* Data Independent Execution Latency (Zkt, Zvkt)

| T_PHY_002
| Physical memory manipulation
| Direct +
Logical or physical
a| * Row-hammer type software attacks to manipulate nearby memory cells
* Using NVDIMM, interposers, or physical probing to read, record, or replay
physical memory
* Physical attacks on hardware shielded locations to extract hardware
provisioned assets
a| * Implement robust memory error detection, cryptographic memory protection,
or physical tamper resistance
* Supervisor domain ID, privilege level, or MTT attributes, may be used to
derive memory encryption contexts at domain or workload granularity
* Provide a degree of tamper resistance

| T_PHY_003
| Boot attacks
| Chained +
Logical or physical
a| * Glitching to bypass secure boot
* Retrieving residual confidential memory after a system reset
a| Implement robust power management, and adopt glitch-safe software techniques. +
 +
Industry best practice should be followed. For example: ensuring un-initialized variables are not used; implementing integrity checking of critical data and hardware provisioned parameters; implementing redundancy in encoding, verification, branching, and critical logic. +
 +
Adopt randomization techniques between boot sessions. For example: cryptographic memory protection with at least boot freshness; register randomization. 

| T_PHY_004
| Subverting supply chains
| Remote
| Infiltration or collusion to subvert security provisioning chains, software
supply chains and signing processes, hardware supply chains, attestation
processes, development processes (for example, unfused development hardware or
debug authorizations)
| Deploy appropriate governance, accreditation, and certification processes for
an ecosystem.

|===

=== Ecosystem security objectives

Ecosystem security objectives identify a set of common features and mechanisms
that can be used to enforce and establish trust in an ecosystem.

These features are defined here at a functional level only. Technical
requirements are typically use case specific and defined by external
certification programs.

In some cases RISC-V non-ISA specifications can provide guidance or protocols.
This is discussed more in use case examples later in this specification.

==== Secure identity

[cat_sr_sub_idn]
[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_IDN_001
| A secure platform MUST be securely identifiable
|===

Identifies the immutable part of the secure platform - immutable hardware,
configurations, and firmware. Immutable components cannot change after
the completion of security provisioning (see also security life cycle management).

A _secure identity_ is an element capable of generating a cryptographic signature
which can be verified by a remote party. This is usually an asymmetric key pair, but
symmetric signing schemes can also be used. Secure identities are typically used as part
of an attestation process.

A secure identity's scope and uniqueness is use case dependent. For example, a secure identity can be:

* Unique to a system
* Shared among multiple systems with the same immutable security properties
(group based anonymization)
* Anonymized using an attestation protocol supporting a third party
anonymization service

A secure identity can be directly hardware provisioned, or derived from other hardware
provisioned assets.

==== Security life cycle

[#cat_sr_sub_lfc]
[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_LFC_001
| A secure system MUST manage a security life cycle.
|===

[caption="Figure {counter:image}: ", reftext="Figure {image}"]
[title= "Generic security life cycle"]
image::img_ch2_security-lifecycle.png[]

[#security-lifecycle]
A security life cycle reflects the trustworthiness of a system during its
lifetime and reflects the life cycle state of hardware provisioned assets.

It can be extended as indicated below to cover additional security provisioning
steps such as device onboarding, device activation, user management, and RMA (Return Merchandize Authorization)
processes. These are use case or ecosystem specific and out of scope of this
specification.

For the purpose of this specification, _revealing debug_ includes any HW or FW
debug capability which:

* Could break security guarantees or could expose assets
* Is not part of an attested trust contract with a relying party

Examples of revealing debug include revealing logging, external debug or
boundary scans, dedicated debug builds of software components, or enabling
self-hosted debug for a component.



Depending on use case, an attested software component can include debug
capabilities managed through an ecosystem defined governance process
- _trusted debug_. For example, self-hosted debug or external debug enabled following an ecosystem
specific authorization process. In this case the debug capability, and the
associated governance, is part of the trust contract with a relying party.

For the purpose of this specification, a minimum security life cycle includes at
least the following states:

* Manufacture - The system may not yet be locked down and has no hardware
provisioned assets
* Security provisioning - The process of provisioning hardware provisioned
assets +
Depending on ecosystem requirement, security provisioning may be performed in
multiple stages through a supply chain and may require additional sub-states.
These types of application specific extensions are out of scope of this
specification.
* Secured - hardware provisioned assets are locked (immutable), only authorized
software can be used, and revealing debug is not enabled. +
Additional specific provisioning stages can take place in this
state - for example network onboarding and device activation, App/Device
attestation or user identity management. This is out of scope of this
specification.
* Recoverable debug - part of the system is in a revealing debug state +
At least the RoT is not compromised and hardware provisioned secrets remain
protected. +
This state is both attestable and recoverable. For example, revealing debug is
enabled for a domain without compromising another domain or any RoT services.
* Terminated - any system change which could expose hardware provisioned assets
+
Typically hardware provisioned assets are made permanently inaccessible and
revoked before entering this state. This also protects any derived assets such
as attestation and sealing keys.

A system may support re-provisioning from a terminated state, for example
following repair/RMA. This can be viewed as equivalent to starting over from the
security provisioning state, and creates a new instance with a new secure
identifier.

[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_LFC_002
| Hardware provisioned assets MUST only be accessible while the system is in
secured state, or a recoverable debug state.(with the recoverable debug state in
attestation evidence).

| SR_LFC_003
| Derived assets MUST only be available if a component is in secured state.
|===

For example, returning garbage or some known test and debug value when attempting to read a hardware provisioned asset, unless the system is in a secured state, or a recoverable debug state. Derived assets would then also become unavailable in these states, though test and debug versions may be available.

A derived asset in this context is any asset derived from hardware provisioned
assets. For example attestation keys, or sealing keys for a supervisor domain.

[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_LFC_004
| Revealing debug MUST be reflected in attestation.

|===

_Attestable states_ are ones where the RoT and hardware provisioned assets are
not compromised by debug and a valid attestation can be generated reflecting
that state:

* Secured
* Recoverable debug

In other states the system is not able to generate a valid attestation key. It
is still _indirectly attestable_ as any generated attestation will not be signed
correctly and can be rejected by a relying party.

Trusted debug is part of a trust contract with a relying party and is application
specific. The presence of trusted debug can be determined indirectly by a
relying party through other attested properties, for example measurements.

==== Attestable services

For the purpose of this specification a confidential service can be any
isolated component on a system. For example, a hosted confidential workload, or
an isolated application security service.

[#cat_sr_sub_att]
[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_ATT_001
| A confidential service, and all software and hardware components it depends
on, MUST be attestable.
|===

Attestation allows a remote relying party to determine the trustworthiness of a
confidential service before submitting assets to it. Attestation aims to:

* Verify the security state of a confidential service
* Verify the security state of all software and hardware a confidential service
depends on
* Establish an attested secure connection to a confidential service

Attestation can be direct or layered:

* Direct +
The whole system can be defined by a single security platform attestation. Eg : vertically integrated connected IoT
devices and edge devices.
* Layered +
Enables parts of the attestation process to be delegated to lower privileged
components.

Direct and layered attestation are discussed in more detail in use case
examples later in this specification.

[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_ATT_002
| A secure platform attestation MUST be signed by a HW RoT, if present, or
else by a FW RoT

| SR_ATT_003
| A secure platform attestation MUST be signed using a hardware provisioned
(directly or derived) secure identity

| SR_ATT_004
| A layered attestation MAY be signed by lower privileged software, itself
attested by a security platform attestation

| SR_ATT_005
a| Layered attestations MUST be cryptographically bound such that a relying
party can determine that they:

* Were generated on the same system
* Are fresh.

|===

NOTE: Software interfaces should only support either direct attestation or
layered attestation workflows, never both, to prevent impersonation attacks.

==== Authorized software

Running unauthorized software can compromise the security state of the system.

[#cat_sr_sub_aut]
[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_AUT_001
| A system in secured or recoverable debug states MUST only load authorized
software.

| SR_AUT_002
| A system in security provisioning state SHOULD only load authorized software.

|===

Two complementary processes can be used to authorize software:

* Measurement +
In the context of this document, a measurement is a record of a present state of the system, which can be used by a remote party to verify the security state of the system. It is typically a cryptographic fingerprint, such as a running hash of memory combined with security lifecycle state and other attributes. Although depending on use case other kinds of measurements can be used.
* Verification +
Verification is a process of establishing that a measurement is correct
(expected)

When a system in a security provisioning state doesnt restrict loading to only authorized software, other protection measures such as physical access protection, or device registration would be required.

A boot process is typically layered, allowing software to be measured and
verified in stages. Different measurement and verification policies can be
employed at different stages. This is discussed further in use case examples
later in this specification. The properties discussed below still apply to each
stage.

NOTE: Measurements can be calculated at boot (_boot state_), and sometimes also
dynamically at runtime (_runtime state_). Measuring runtime state can be used as
a robustness feature to mitigate against unauthorized runtime changes of static
code segments. It is out of scope of this specification, though the principles
discussed below can still be applied.

Verification can be:

* Local +
A measurement is verified locally on the device.
* Remote +
A measurement is verified by a remote provisioning service, or a remote relying
party.

Verification can be:

* Direct +
The measurement is directly compared with an expected measurement from a signed
authorization.
* Indirect +
The measurement is included in derivations of other assets, for example sealing
keys, binding assets to a measured state.

[#cat_sr_sub_msm]
[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_MSM_001
| A secure platform MUST be measured.

| SR_MSM_002
| A secure platform MUST be verified, either directly or indirectly, before
launching services which depend on the security platform.

|===

Verification ensures the system has loaded authorized software

[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_MSM_003
| A system MUST only use authorizations from trusted authority.
|===

* Direct verification requires a signed image authorization from a trusted
authority before loading an image +
For example, a signed image, or a separately signed authorization
message.
* Indirect verification requires a signed authorization from a trusted authority
for migrating assets bound to a previously measured state +
For example, a signed provisioning message.

Either way, only authorizations from trusted authorities should be used. For
example, from a list of hardware provisioned or securely discovered trusted
authorities.

[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_MSM_004
| Local verification MUST be rooted in immutable boot code.
|===

For example, ROM or locked flash, or rooted in a HW RoT itself rooted in
immutable boot code.

==== System updates

Over time, any mutable component may need updates to address
vulnerabilities or functionality improvements. A system update can concern
software, firmware, microcode, or any other updatable component on a system.

[#cat_sr_sub_upd]
[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_UPD_001
| All components on a system which are not immutable MUST be updatable.
|===

Immutable components include at least immutable boot code. Some trusted
subsystems can also include immutable software to meet specific security
certification requirements.

System updates are typically layered so that updates can target only parts of a
system and not a whole system. The properties discussed below still apply to
any system update.

[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_UPD_002
| A system update MUST be measured and verified before launch.
|===

See <<_authorized_software>>.

A system update can be:

* Deferred +
The update can only be effected after a restart of at least the affected
component, and all of its dependents.
* Live +
The update can be effected without restarting any dependent components.

[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_UPD_003
| Updates affecting a security platform SHOULD be deferred.

| SR_UPD_004
| Updates MAY be live if live update capability, and suitable governance, is
part of an already attested trust contract between a relying party and the
system.
|===

A system update changes the attested security state of the affected
component(s), as well as that of all other components that depend on it. It can
affect whether a dependent confidential service is still considered trustworthy
or not, as well as affect any derived assets such as sealing keys.

[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_UPD_005
| System updates MUST be monotonic

| SR_UPD_006
| System updates SHOULD be robust against update failures
|===

Earlier versions could be carrying known vulnerabilities, or could be able to affect the safe
operation of a system in other ways.

For example, using derived anti-rollback counters (counter tree) rooted in a
hardware monotonic counter.

A system can still support recovery mechanisms, with suitable governance, in
the case of update failures. For example, a fallback process or a dedicated
recovery loader.

Success criteria for a system update are typically use case or ecosystem
specific and out of scope of this specification. Examples include local
watchdog or checkpoints, and network control through a secure update protocol,
and a dedicated recovery loader.

[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_UPD_007
| System updates, and authorization messages, SHOULD only be received from
trusted sources.

|===

A system update is itself always verified before being launched. Verifying the
source as well can mitigate against attempts to inject adversary controlled
data into a local update process. Including into protected memory regions.

==== Isolation
Complex systems include software components from different supply chains, and
complex integration chains with different roles and actors. These supply chains
and integration actors often share mutual distrust:

* Developed, certified, deployed and attested independently
* Protected from errors in, or abuse from, other components
* Protected from debugging of other components
* Contain assets which should not be available to other components

Use cases later in this specification provide examples of RISC-V isolation
models.

[#cat_sr_sub_iso]
[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_ISO_001
| Isolated software components SHOULD be supported
|===

An isolated component has private memory and private execution contexts not
accessible to other components.

[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_ISO_002
| Devices MUST not access memory belonging to an isolated component without
permission
|===

Isolation can also extend to other features, such as interrupts and debug.

==== Sealing

Sealing is the process of protecting confidential assets on a system, typically
using sealing keys derived in different ways for different use cases as
discussed in this section. For example, from a hardware provisioned root key,
from a boot state (measurements, security life cycle state), or provisioned at
runtime by a remote provisioning system.

Sealing can be:

* Local +
Local sealing binds assets to a local device (hardware unique sealing) or to a
measured boot state.
* Remote +
Remote sealing binds assets to credentials provided by a remote provisioning
service following successful attestation.

[#cat_sr_sub_slg]
[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_SLG_001
| Sealed assets SHOULD only be possible to unseal in a secured state

|===

For example, local sealing key derivations should take the security life cycle
state of the system into account. And remote sealing key provisioning should
always attest the system before releasing unsealing credentials or keys.

Local sealing can be:

* Direct +
Direct sealing binds assets to sealing keys derived by a RoT.
* Layered +
 Layered sealing enables delegation of some sealing key derivations to lower
privileged software.

[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_SLG_002
| Locally sealed assets MUST only be possible to unseal on the same physical
instance of a system that they were sealed on.

|===

For example, using sealing keys derived from a hardware provisioned _hardware
unique key (HUK)_.

[width=100%]
[%header, cols="5,20"]
|===
| ID#
| Requirement

| SR_SLG_003
| Locally sealed assets bound to a boot measurement MUST only be possible to
unseal if that measurement has not changed, or the system has received an
authorized update.

|===

See <<_system_updates, system updates>>

Sealing is discussed further in use cases examples later in this document.
