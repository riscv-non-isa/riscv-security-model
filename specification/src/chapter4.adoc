:imagesdir: ../images

[[chapter4]]

== Use Case Examples - Non Normative

This chapter provides a selection of non-exhaustive example security usecases based on commonly used
deployment security models. The examples may be extended over time as required. This section is non normative
guidance only, detailing the recommended combination of security requirements for the associated use case.

For ease of reference, for each usecase, <<R2>> lists the mapping of the RISC-V
ISA and non-ISA building blocks used along with links to the building block
specification, POCs, and code repositories available.

=== Generic system without supervisor domains

==== Overview

[caption="Figure {counter:image}: ", reftext="Figure {image}"]
[title= "Generic vertically integrated system"]
image::img_ch4_priv.png[]

A generic vertically integrated system can be either virtualized or
non-virtualized.


M-mode hosts a FW RoT. An OS or a Hypervisor in S or HS mode controls
applications or guests. Guests and applications execute in U or VS/VU modes and
trust the OS or Hypervisor to provide isolation guarantees.


NOTE: The RISC-V architecture also caters for systems with just M and U modes,
commonly used in embedded systems, helper cores, and similar use cases. On
secure systems not supporting S-mode a FW RoT has to share M-mode with an OS.
RISC-V does not exclude such implementations - for example, implementations
using a certified OS and FW RoT, or using a HW RoT to isolate sensitive code
and assets (physical isolation). There is no current mechanism in RISC-V for
isolation within M-mode itself other than temporal boundaries +
 +
To minimize the TCB of the FW RoT RISC-V recommends that secure systems
implement S mode, and deprivilege non-RoT firmware such as an OS or
non-security services.

==== Isolation model

[width=100%]
[%header, cols="5,20"]
|===
| Requirement
| Reference

| Protect M-mode from lower privilege levels.
| SR_PMP_001, SR_PMP_002

| Protect Supervisor mode from lower privilege levels (with or without H-extension)
| SR_PMP_004

|===

See xref:chapter3.adoc#_pmp_and_epmp[PMP and Smepmp]

See xref:chapter3.adoc#_spmp[sPMP]

See xref:chapter3.adoc#_mmu[MMU]

The sPMP is typically used in relatively static deployments such as determinism sensitive use cases like automotives.

The MMU is typically required for Linux based systems and for virtualized systems.

Either MMU or sPMP can be used both with or without the hypervisor extension. For
example, the hypervisor extension with sPMP can support static partitioning
hypervisors commonly used in automotive. A single stage MMU can be used
without the hypervisor extension for full Linux support.

==== Root of Trust

See xref:chapter2.adoc#_reference_model[reference model].

[width=100%]
[%header, cols="5,20"]
|===
| Requirement
| Reference

| A system level HW RoT is recommended
| SR_ROT_001,
SR_ROT_002

|===


==== Authorized Boot

Multiple models can be used to ensure a secure system can only run authorized
software.

See xref:chapter2.adoc#_authorized_software[authorized software].

==== Attestation

Multiple models can be used to prove to a relying party that a secure system is
in a trustworthy state.

See xref:chapter2.adoc#_attestable_services[attestable services].

==== Sealing

Multiple models can be used to protect assets if a system is not in a
trustworthy state.

See xref:chapter2.adoc#_sealing[sealing].

==== Device access control

For the purpose of this specification, a device can be a logical device. A
physical device can present one or more logical devices, each with its own
(logical) control interface.

Isolation guarantees provided to software also apply to device initiated
transaction.

[width=100%]
[%header, cols="1,^1"]
|===
| Requirement | Reference

| Guarantee that devices assigned to lower
  privilege levels cannot access resources
  assigned to M-mode.
|  SR_IOM_005, +
  (SR_IOP_001, SR_IOP_002, SR_IOP_003) +
  OR +
  (SR_IOM_001, SR_IOM_002, SR_IOM_003) +


| Enforce access rules for devices assigned
  to user applications or guests on a virtualized
  system.
| SR_IOP_004 OR SR_IOM_004

|===

On a non-virtualized system, user devices can be managed by the OS which can
enforce access rules for user applications.

On a virtualized system, devices can be virtualized and assigned to guests by
the hypervisor configuring MMU and IOMMU translation rules.

NOTE: IOMTT can also be sufficient for protecting Root devices in the sense that
M-mode can enforce that its own resources are never assigned to another domain.
Use of IOPMP or similar could still add further protections. For example, a system
may require that Root devices cannot access memory assigned to Confidential
domain.

==== Debug and performance management

See xref:chapter2.adoc#_security_lifecycle[security life cycle]. +
See https://github.com/riscv-non-isa/riscv-external-debug-security[RISC-V external debug security]

[width=100%]
[%header, cols="5,20"]
|===
| Requirement
| Reference

| Securely control debug access
| SR_DBG_001

| Allow Debug of non-M-mode software while blocking debug of higher privilege code
| SR_DBG_002

| Allow Self-hosted Debug of non M-mode software
| SR_DBG_003, SR_DBG_004

|===

For example, external debug can be enabled for non-M-mode software without affecting M-mode (recoverable debug). And an S-mode OS can enable self-hosted debug for a user application without affecting other applications or S-mode itself.

[width=100%]
[%header, cols="5,20"]
|===
| Requirement
| Reference

| Allow a FW RoT to prevent debug of a production system
| SR_DBG_005

|===

For example, disable self-hosted debug in a production system for certification
reasons.

[width=100%]
[%header, cols="5,20"]
|===
| Requirement
| Reference

| Include debug controls in boot time measurement for attestation purpose.
| SR_DBG_006, SR_DBG_007, SR_DBG_008, SR_LFC_004

|===

Guarantees the system remains attestable.

[width=100%]
[%header, cols="5,20"]
|===
| Requirement
| Reference

| Protect an application or domain against monitoring without consent or DOS by other applications or domains
| SR_PMU_001, SR_PMU_002, SR_QOS_001, SR_QOS_002, SR_DOS_001, SR_DOS_002

|===

Prevents using event counters to monitor across application or privilege
boundaries. Event counters can be managed by higher privileged software as part
of context switching across boundaries.

=== Global Platform TEE

==== Overview

[caption="Figure {counter:image}: ", reftext="Figure {image}"]
[title= "Global platform TEE use cases"]
image::img_ch4_gp-tee.png[]

https://globalplatform.org/[Global platform] defines technical standards,
interface specifications and programming models, open source firmware, and
certification programs for _trusted execution environments (TEE)_.

A TEE is an isolated environment providing security services. TEE services can
be available to software on multiple Harts. For example:

* Payment clients
* DRM clients and content protection
* Secure storage
* User identity management
* Attestation services

The TEE model divides software into physically isolated domains:

* Normal domain +
Typically hosting a _rich OS_ (for example, RTOS or Linux), and user
applications.
* TEE domain +
Hosts a _TEE OS_ (domain security manager) and _trusted applications (TA)_.
* Root domain +
Hosts RoT firmware, including a secure monitor.

The TEE OS is primarily responsible for isolation of TA, and for providing root
of trust services, within the TEE domain.

The OS in Normal domain typically controls scheduling on the system, across all
Harts available to it. To interact with TA services in TEE domain, the OS in
Normal domain interacts with a TEE OS through a secure monitor in Root domain.

The secure monitor is responsible for context switching and isolation across
domain boundaries, including event management.

For the purpose of this specification, TEE deployment models can be separated
as:

* Static partitioning TEE +
A single TEE provides security services to Normal domain. TA are typically
installed at boot by RoT FW and TEE OS, though Global Platform does also define
protocols for installation of TA at runtime. System configuration and resource
allocation can be mostly static, making the system more deterministic. +
 +
_Use case examples:_ edge devices and IoT, automation, and automotive.
* Virtualized TEE +
On a virtualized system, TEE can also be virtualized. In this case a _secure
partition manager_ (SPM) in TEE domain is responsible for isolation of multiple TEE
guests (for example, an OEM TEE and separate third party TEE). This model can
also support more dynamic resource allocation. +
 +
_Use case examples:_ mobile clients, and automotive.

==== Isolation model

A Global Platform TEE requires the following isolation guarantees:

[width=100%]
[%header, cols="5,20"]
|===
| Requirement
| Reference

| Allow Root domain to access resources assigned to any domain, while preventing
itself from unintended access to resources assigned to a different domain
(privilege escalation).
| SR_PMP_003

| Prevent other domains from accessing resources assigned to Root domain
| (SR_PMP_001, SR_PMP002) OR (SR_MPT_001, SR_MPT_002), +
  SR_SUD_001


| Block resources assigned to TEE domain from access by Normal domain
| SR_SUD_001, SR_SUD_002, SR_SUD_003

| Allow resources assigned to Normal domain to be accessible to Normal domain
(r/w/x), and to TEE domain (r/w) (default sharing rule)
| SR_SUD_004

| Ensure resources assigned to a single TA, or a guest TEE, are not be accessible by a
different TA, or guest TEE, without consent.
| SR_PMP_005 OR SR_MMU_003

|===

In the standard GP TEE model, each TA is expected to be a self-contained unit
providing a specific security service, either to Normal domain or to other TA.
All communications are implemented through secure channels managed by the TEE OS
or SPM.

Sharing of memory between TA is generally discouraged. But there are mechanisms
to do so in specific use cases. For example, sharing media buffers in a secure
media path. Such policies are enforced by SPM or TEE OS.

Processes in Normal domain can share memory assigned to Normal domain when
interacting with a TA in TEE world (default sharing rule). Such shared memory
can be cached when context switching between Normal and TEE domains.

RISC-V hardware enforced isolation mechanisms can be used as follows to meet
those guarantees:

See xref:chapter3.adoc#_supervisor_domains[supervisor domains].
See xref:chapter3.adoc#_pmp_and_epmp[PMP and Smepmp]
See xref:chapter3.adoc#_spmp[sPMP]
See xref:chapter3.adoc#_mmu[MMU]
See xref:chapter3.adoc#_mtt[MTT]

[width=100%]
[%header, cols="5,20"]
|===
| Requirement
| Reference

| Use Supervisor domains to enforce isolation between Normal and TEE domains, and to protect machine mode from other domains
| SR_SUD_001, SR_MPT_001, SR_MPT_002

| For a static partition TEE, use PMP, sPMP, MMU or MPT to enforce isolation
between TA in TEE domain.
| SR_PMP_005 OR SR_MMU_003

| For a virtualized TEE, use hypervisor extension
| SR_HYP_001,SR_MMU_001, SR_MMU_002,

| For a virtualized TEE, sPMP or MMU MUST be used to enforce isolation between guest
TEE, and between TA within a TEE.
| SR_PMP_005 OR SR_MMU_003

|===

==== Root of Trust

See xref:chapter2.adoc#_reference_model[reference model].

[width=100%]
[%header, cols="5,20"]
|===
| Requirement
| Reference

| It is recommended for a TEE based system implement a HW RoT
| SR_TOT_001, SR_ROT_002

|===

==== Authorized boot

See xref:chapter2.adoc#_authorized_software[authorized software].

TEE boot is typically based on:

* Measured and verified local boot (direct or indirect)
* Sealing, to protect TEE production assets

The process can involve multiple stages (layered boot).

[width=100%]
[%header, cols="5,20"]
|===
| Requirement
| Reference

| Direct or indirect measurement of a system verifies the software is authorised
| SR_MSM_001, SR_MSM_002, SR_MSM_003

| Immutable code ensures a trusted starting point
| SR_MSM_004

| Systems allow secure updates to all mutable components
| SR_UPD_001, SR_UPD_002, SR_UPD_005, SR_UPD_006, SR_UPD_007

|===


==== Attestation

See xref:chapter2.adoc#_attestable_services[attestable services].

Static partition TEE attestation is typically based on a direct security
platform attestation.

[width=100%]
[%header, cols="5,20"]
|===
| Requirement
| Reference

| Attestation is used to determine trustworthyness across all comonents
* TEE domain
* Root domain
* Boot state of all trusted subsystems
| SR_ATT_001, SR_ATT_002, SR_ATT_003

|===

Virtualized TEE attestation can be layered, for performance or separation of
concern. For example:

* A security platform attestation, signed by a RoT, covering trusted subsystems,
Root domain, and SPM
* Separate guest TEE attestation(s) signed by SPM

[width=100%]
[%header, cols="5,20"]
|===
| Requirement
| Reference

| Layered attestation allows delegation in complex systems
| SR_ATT_004, SR_ATT_005
|===

==== Sealing

See xref:chapter2.adoc#_sealing[sealing].

In the Global Platform security model, SPM or TEE OS typically provide local
trusted storage, key management, and cryptographic services to TA and guest TEE.
These services support local sealing of TA or guest TEE assets, and minimize
exposure of cryptographic materials.

[width=100%]
[%header, cols="5,20"]
|===
| Requirement
| Reference

| Local sealing for a TA, or a TEE guest, must be unique to TEE domain and to a
physical instance of a system.

| SR_SUD_002, SR_SLG_002

| Local sealing for a TA, or a TEE guest, should also be unique to the TEE guest
or the TA.Local sealing MAY be layered

| SR_MMU_003 OR SR_PMP_005
|===

For example:

* TEE domain unique sealing keys derived by a RoT from a hardware unique key
* TA, or guest TEE, unique sealing keys derived by TEE OS or SPM from a TEE
domain unique sealing key

==== Device access control

For the purpose of this specification, a device can be a logical device. A
physical device can present one or more logical devices, each with its own
(logical) control interface.

The security guarantees also apply to device initiated accesses, for example DMA
and interrupts.

[width=100%]
[%header, cols="5,20"]
|===
| Requirement
| Reference

| A static partition TEE must use IOPMP to enforce access rules for devices.
| SR_IOP_004

| A virtualized TEE must use IOMPT and IOMMU to enforce access rules for devices
assigned to Normal or TEE domains, and should use IOPMP to enforce access rules
for Root devices.
| SR_IOM_001, SR_IOM_002, SR_IOM_003, SR_IOM_004, SR_IOM_005
|===

For a static partition TEE, domain level granularity can be sufficient as device
access within TEE and Normal domains is governed by TEE OS and the rich OS
respectively. It can be implemented using IOPMP. Policy can be controlled by
boot configuration, by a HW or FW RoT.

For a virtualized TEE, IOMTT enforces supervisor domain level access rules
(physical isolation). IOMMU enforces guest and TA level access rules
(virtualization), supporting device assignment to a guest TEE or a TA.

NOTE: IOMTT can also be sufficient for protecting Root devices in the sense that
M-mode can enforce that its own resources are never assigned to another domain.
Use of IOPMP or similar could still add further protections. For example, a system
may require that Root devices cannot be used to access memory assigned to
Confidential domain.

==== System integration

In the case of a Global Platform TEE system a rich OS in Normal domain is free
to schedule services, including TEE services, on any Hart available to it. The
number and make-up of supervisor domains can be known, and a simple convention
can be used for common identification (SDID value, see
xref:chapter3.adoc#_supervisor_domains[supervisor domains]) of Normal, TEE, and
Root domains across multiple Harts in a system.

System integration in this context involves providing _security attributes_ on
a system interconnect, tagging all transactions (CPU or system agent initiated)
to either Root, Normal, or TEE domains.

Possible use cases include:

* Tweaking cryptographic memory protection (uniqueness)
* Tagging interrupts, debug accesses, or coherent memory accesses
* Device assignment (IOPMP/IOMTT integration), static or dynamic

The attributes can be derived, for example, from SDID and privilege level, or from
PMA.

For some use cases security attributes can be extended to reflect finer
granularity, for example for cryptographic memory protection with TA
granularity.

==== Debug and performance management

See xref:chapter2.adoc#_security_lifecycle[security life cycle]. +
See https://github.com/riscv-non-isa/riscv-external-debug-security[enhanced RISC-V external debug security]

[width=100%]
[%header, cols="5,20"]
|===
| Requirement
| Reference

| External debug must be enabled separately for Root domain.
| SR_DBG_001, SR_DBG_002

| External debug must be enabled separately for each supervisor domain.
| SR_SUD_005

| External debug must only be enabled by a HW RoT (Root domain external debug)
or by Root domain (supervisor domain external debug).
| SR_DBG_001, SR_SUD_005

| Self-hosted debug may be used for debug within a supervisor domain.
| SR_DBG_003

| Self-hosted debug must only be enabled by a higher privileged component.
| SR_DBG_004
|===

For example, within normal domain an S-mode or VS-mode OS can enable
self-hosted debug for a user application. Or an HS-mode hypervisor can enable
self-hosted debug for a VS-mode guest. Only Root domain should enable
self-hosted debug for an S-mode OS or an HS mode hypervisor.

Within TEE domain a TEE OS can enable self-hosted debug for a TA. An SPM can
enable self-hosted debug for guest TEE. Only Root domain should enable
self-hosted debug of SPM (virtualized) or TEE OS (non-virtualized).

A machine mode monitor can enable external debug of individual supervisor domains without affecting M-mode, or any other supervisor domain.

[width=100%]
[%header, cols="5,20"]
|===
| Requirement
| Reference

| Root domain may disable self-hosted debug for a whole domain.
| SR_DBG_005
|===

For example, for all of TEE domain on a production system, for certification
reasons.

[width=100%]
[%header, cols="5,20"]
|===
| Requirement
| Reference

| External debug MUST only be enabled following system reset (part of measuring)
of the affected component.
|SR_DBG_006

| Revealing self-hosted debug MUST only be enabled following reboot (part of
measuring) of the affected component.
|SR_DBG_007

| Trusted self-hosted debug MAY be enabled at runtime (after measuring) of the
affected component, to an application specific governance process.
|SR_DBG_008
|===

Guarantees the system remains attestable.

See xref:chapter2.adoc#_event_counters[event counters]

=== Confidential computing on RISC-V (CoVE)
==== Overview
[caption="Figure {counter:image}: ", reftext="Figure {image}"]
[title= "Confidential compute use case"]
image::img_ch4_cove.png[]

In hosting environments, tenant workloads rely on isolation primitives that are
managed by host privileged software. This can lead to a large TCB for tenants
which may include, for example, a hypervisor, orchestration services, and
host management services. It may also include other tenants exploiting
vulnerabilities in complex hosting software.

Confidential compute aims to achieve a minimal and certifiable TCB for
_confidential workloads_.

_CoVE (Confidential VM Extensions)_
https://github.com/riscv-non-isa/riscv-ap-tee/tree/main/specification[specification]
defines a confidential compute platform for RISC-V systems, including
interfaces and programming models, covering life cycle management, attestation,
resource management and devices assignment, for confidential workloads. It is
based on principles defined by
https://confidentialcomputing.io/[Confidential Computing Consortium].
Reference firmware for CoVE is being developed as part of the
https://riseproject.dev/[RISC-V Software Ecosystem] project.

CoVE is primarily aimed at cloud hosting of confidential workloads. In this deployment model
CoVE divides software into physically isolated domains:

* Normal domain +
Typically hosting a hypervisor, and Normal guests and services.
* Confidential domain +
Hosts a domain security manager (_trusted security manager, TSM_) and confidential guests.
* Root domain +
Hosts RoT firmware, including a secure monitor.

The TSM is primarily responsible for isolation of confidential workloads, and
for providing RoT services, within the Confidential domain.

A hypervisor in Normal domain typically controls scheduling and resource
assignment on the system across all Harts available to it, including for
confidential workloads. It interacts with the TSM through the secure monitor in
Root domain to manage confidential workloads.

The secure monitor is responsible for context switching and isolation across
domain boundaries, including event management.

More details including a threat model and the security requirements to address that threat
model can be found in the _CoVE (Confidential VM Extensions)_
https://github.com/riscv-non-isa/riscv-ap-tee/tree/main/specification[specification]

The information below adds cross references to the security model normative security requirements.

The underlying isolation mechanisms may be used in other deployment models, such
as some mobile clients or edge devices whose design may might be constrained by real
time and formal verification requirements. The TSM and secure monitor function are
then combined into a single TEE security manager in Root domain.


==== Isolation model

Confidential workloads require isolation guarantees. RISC-V hardware enforced isolation mechanisms can be used as follows to meet those requirments:

See xref:chapter3.adoc#_supervisor_domains[supervisor domains].
See xref:chapter3.adoc#_pmp_and_epmp[PMP and Smepmp]
See xref:chapter3.adoc#_spmp[sPMP]
See xref:chapter3.adoc#_mmu[MMU]
See xref:chapter3.adoc#_mtt[MTT]

[width=100%]
[%header, cols="5,20"]
|===
| Requirement
| Reference


| Allow Root domain to access resources assigned to any domain, while preventing
itself from unintended access to resources assigned to a different domain
(privilege escalation).
| SR_PMP_003

| Prevent other domains from accessing resources assigned to Root domain
| (SR_PMP_001, SR_PMP002) OR (SR_MPT_001, SR_MPT_002), +
  SR_SUD_001

| Block resources assigned to Confidential domains from access by Normal domain
| SR_SUD_001, SR_SUD_002, SR_SUD_003, SR_MMU_001, SR_MMU_002

| Block resources assigned to Normal domain from access by Confidential domain
| SR_SUD_001, SR_SUD_002, SR_SUD_003, SR_MMU_001, SR_MMU_002

| Allow resources to be assigned to both Normal domain and Confidential domain (sharing by consent)
| SR_SUD_001, SR_SUD_002, SR_SUD_003

| Ensure resources assigned to a confidential workload are not be accessible by other confidential worloads
without consent.
| SR_MMU_003

| Implement hypervisor functionality for resource management
| SR_HYP_001, SR_MMU_001, SR_MMU_002


|===


==== Root of trust

See xref:chapter2.adoc#_reference_model[reference model].

[width=100%]
[%header, cols="5,20"]
|===
| Requirement
| Reference

| Implement a HW RoT
| SR_TOT_001, SR_ROT_002

|===

==== Authorized Boot

See xref:chapter2.adoc#_authorized_software[authorized software].

Boot in a cloud hosting context is typically based on:

* Measured boot of a hosting platform, including Root domain and TSM
* Platform attestation and security provisioning (unsealing) by a remote
provisioning system
* Launch and measurement of confidential workloads, only once the system has
been unsealed

A _trusted platform module_ (TPM) can be used to measure the security platform.

Measuring confidential guests can be done by TSM in Confidential domain.

The process can involve multiple stages (layered boot).

[width=100%]
[%header, cols="5,20"]
|===
| Requirement
| Reference

a| Confidential guests must not boot until at least the security platform has
been verified:

* TSM in Confidential domain
* Root domain
* Boot state of all trusted subsystems

Direct or indirect measurement of a system verifies the software is authorised
| SR_MSM_001, SR_MSM_002, SR_MSM_003

| Immutable code ensures a trusted starting point
| SR_MSM_004

| Systems allow secure updates to all mutable components
| SR_UPD_001, SR_UPD_002, SR_UPD_005, SR_UPD_006, SR_UPD_007

|===

==== Attestation

See xref:chapter2.adoc#_attestable_services[attestable services].

Virtualized TEE attestation can be layered, for performance or separation of
concern. For example:

* A security platform attestation, signed by a RoT, covering trusted subsystems,
Root domain, and SPM
* Separate guest TEE attestation(s) signed by SPM


See xref:chapter2.adoc#_attestable_services[attestable services].

Attestation of confidential workloads is typically layered, for performance and
separation of concern:

* A security platform attestation, signed by a hardware root of trust
* A confidential workload attestation, signed by TSM

[width=100%]
[%header, cols="5,20"]
|===
| Requirement
| Reference

a| A security platform attestation is used, covering at least:

* HW RoT
* TSM
* Root domain
* Boot state of all trusted subsystems
| SR_ATT_001, SR_ATT_002, SR_ATT_003

|===

==== Sealing

See xref:chapter2.adoc#_sealing[sealing].

Sealing of confidential workloads is typically based on remote sealing,
unsealing assets for a confidential workload following successful attestation
by a remote provisioning system. This enables use cases such as:

* Shared assets across multiple instances of a confidential workload (scale or
redundancy)
* Unsealing different sets of assets for different users of a service

TSM itself is typically stateless across reset and does not require any sealed
assets of its own.

[#_cove_device_access_control]
==== Device access control

For the purpose of this specification, a device can be a logical device. A
physical device can present more than one logical devices, each with its own
(logical) control interface.

The security guarantees also apply to device initiated accesses, for example
DMA and interrupts.

[width=100%]
[%header, cols="5,20"]
|===
| Requirement
| Reference

| IOMPT and IOMMU are used to enforce access rules for devices assigned to
Normal or Confidential domains, and IOPMP is used to enforce access rules
for Root devices.
|SR_IOM_001, SR_IOM_002, SR_IOM_003, SR_IOM_004, SR_IOM_005

| IOPMP and IOMPT configurations are only directly accessible by
Root domain.
| SR_PMP_001, SR_IOM_002

|===

IOMTT enforces supervisor domain level access rules (physical isolation).
IOMMU enforces guest and TA level access rules (virtualization), supporting
device assignment to a Confidential guest.

NOTE: IOMTT can also be sufficient for protecting Root devices in the sense
that M-mode can enforce that its own resources are never assigned to another
domain. Use of IOPMP or similar could still add further protections. For example,
a system may require that Root devices cannot be used to access memory assigned
to Confidential domain.

==== System integration

In the case of a confidential compute system, hypervisor in Normal domain
typically controls scheduling and resource assignment on the system across all
Harts available to it. The number and make-up of supervisor domains can be
known, and a simple convention can be used for common identification of Normal,
Confidential, and Root domains across multiple Harts in a system.

System integration in this context involves providing _security attributes_ on
the interconnect, tagging all transactions (CPU or system agent initiated) to
either Root, Normal, or TEE domains.

Possible use cases include:

* Tweaking cryptographic memory protection (uniqueness)
* Tagging interrupts, debug accesses, or coherent memory accesses
* Device assignment (IOPMP/IOMTT integration), static or dynamic

The attributes can be derived, for example, from SDID and privilege mode.

For some use cases security attributes can be extended to reflect finer
granularity, for example for cryptographic memory protection with confidential
workload granularity.

==== Trusted device assignment

The goal of confidential compute is to provide a minimum TCB for a confidential
service, and CPU isolation mechanisms discussed so far does that on a Hart.

But most confidential services also make use of devices, both on-chip and
external. <<_cove_device_access_control, Device virtualization>> can guarantee
exclusivity for devices assigned to a confidential workload - TSM can guarantee
that a device assigned to a confidential workload cannot be accessed by:

* Any other confidential workload
* Any software in Normal domain

But the confidential workload still has to trust all intermediaries between the
workload and the device, both physical and software. For example:

* Drivers
* Physical interconnects and device hardware interfaces

Secure access to devices is important in a number of use cases where a device
performs work on assets owned by a confidential workload, such as accelerators.

The _TEE device interface security protocol (TDISP)_ defined by PCIe provides a
security architecture and protocols allowing a confidential workload to
securely attest, manage and exchange data with a trusted device.

CoVE defines RISC-V support for TDISP. See:

https://pcisig.com/specifications/
https://github.com/riscv-non-isa/riscv-ap-tee-io

==== Debug and performance management

See xref:chapter2.adoc#_security_lifecycle[security life cycle]. +
See https://github.com/riscv-non-isa/riscv-external-debug-security[enhanced RISC-V external debug security]

[width=100%]
[%header, cols="5,20"]
|===
| Requirement
| Reference

| External debug must be enabled separately for Root domain.
| SR_DBG_001, SR_DBG_002

| External debug must be enabled separately for each supervisor domain.
| SR_SUD_005

| External debug must only be enabled by a HW RoT (Root domain external debug)
or by Root domain (supervisor domain external debug).
| SR_DBG_001, SR_SUD_005

| Self-hosted debug may be used for debug within a supervisor domain.
| SR_DBG_003

| Self-hosted debug must only be enabled by a higher privileged component.
| SR_DBG_004

|===

For example, within normal domain an HS-mode hypervisor can enable self-hosted
debug for a VS-mode guest. Only Root domain should enable self-hosted debug for
the HS mode hypervisor.

Within Confidential domain the TSM can enable self-hosted debug for a
confidential guest. Only Root domain should enable self-hosted debug of TSM.

A machine mode monitor can enable external debug of individual supervisor domains without affecting M-mode, or any other supervisor domain.

[width=100%]
[%header, cols="5,20"]
|===
| Requirement
| Reference

| External debug must only be enabled following system reset (part of measuring)
of the affected component.
| SR_DBG_006

| Revealing self-hosted debug must only be enabled following reboot (part of
measuring) of the affected component.
| SR_DBG_007

| Trusted self-hosted debug may be enabled at runtime (after measuring) of the
affected component, to an application specific governance process.
| SR_DBG_008

|===

Guarantees the system remains attestable.

See xref:chapter2.adoc#_event_counters[event counters]

==== Platform QoS

See xref:chapter2.adoc#_platform_quality_of_service[platform quality of service].
